# AQA-README: Описание автотестов для Translator

## Общая информация

В этом проекте реализован полный набор автотестов для проверки логики приложения `src/app.py` (Flask + интеграция с внешним LLM API). Тесты написаны на `pytest`, не используют моки — вместо этого поднимается локальный HTTP-сервер, имитирующий внешний API для максимально реалистичного тестирования.

**Единый файл тестов:** `tests/unit/test_all.py` (11 тестов)

## Как запускать тесты

1. Установите зависимости:
   ```bash
   python -m pip install -r requirements.txt
   ```

2. Запустите все тесты:
   ```bash
   pytest tests/unit/test_all.py -v
   ```
   или просто:
   ```bash
   pytest tests/unit -q
   ```

3. Запустите конкретный тест:
   ```bash
   pytest tests/unit/test_all.py::test_positive_call_llm -v
   ```

## Какие функции приложения покрываются

- **`call_llm(model_name, prompt)`** — основная функция для обращения к внешнему LLM API
- **Загрузка конфигурации** — API-ключ и URL из переменных окружения
- **Обработка ошибок** — сетевые ошибки, невалидный JSON, ошибки HTTP
- **HTTP-протокол** — заголовки (Authorization), структура JSON-тела, таймауты
- **Веб-интерфейс** — обработка POST/GET, рендеринг HTML-шаблона, интеграция с LLM

## Описание тестов

### 1. **test_positive_call_llm**
**Что проверяет:** Успешный вызов API (HTTP 200) и получение ответа.
- Отправляет запрос к LLM API
- Проверяет, что возвращается текст из поля `response`
- **Сценарий:** Happy path — всё работает корректно

### 2. **test_env_loading_api_key**
**Что проверяет:** Загрузка API-ключа из переменных окружения.
- Устанавливает `MENTORPIECE_API_KEY` в окружение
- Перезагружает модуль `app`
- Проверяет, что `API_KEY` содержит корректное значение

### 3. **test_error_handling_call_llm**
**Что проверяет:** Обработка ошибок от API (HTTP 500).
- API возвращает ошибку (статус 500)
- Функция не падает, а возвращает строку с сообщением об ошибке
- **Сценарий:** Error handling — приложение устойчиво к сбоям API

### 4. **test_authorization_header_and_body_structure**
**Что проверяет:** Корректность HTTP-запроса к API.
- Заголовок `Authorization: Bearer <API_KEY>` присутствует
- JSON-тело содержит поля `model_name` и `prompt` с правильными значениями
- **Сценарий:** Request validation — проверка протокола взаимодействия

### 5. **test_flask_route_post_integration_and_template_presence**
**Что проверяет:** Интеграция Flask и рендеринг HTML-шаблона.
- Отправляет POST на `/` с данными формы
- Проверяет статус ответа (200)
- Проверяет наличие элементов в HTML: `<textarea>`, `<select>`, текст "Перевод"
- Проверяет, что результат переводу отображается в ответе
- **Сценарий:** Web UI — полный цикл работы веб-интерфейса

### 6. **test_missing_api_key_handling**
**Что проверяет:** Поведение при пустом API-ключе.
- Устанавливает `MENTORPIECE_API_KEY` в пустую строку
- Функция отправляет запрос с заголовком `Authorization: Bearer ` (пустой ключ)
- Приложение не падает, обрабатывает ошибку корректно
- **Сценарий:** Edge case — отсутствие ключа

### 7. **test_connection_refused_handling**
**Что проверяет:** Обработка недоступности API.
- API URL указывает на порт, где ничего не слушает
- Функция возвращает строку об ошибке (не выбрасывает исключение)
- **Сценарий:** Network error — сервер недоступен

### 8. **test_edge_cases_empty_and_long_input**
**Что проверяет:** Граничные случаи по размеру ввода.
- **Пустой ввод:** Отправляется пустая строка, API отвечает корректно
- **Длинный ввод:** 20 000 символов отправляются успешно
- Приложение не падает при любых размерах
- **Сценарий:** Performance и stability — устойчивость к крайним значениям

### 9. **test_language_selection_prompt**
**Что проверяет:** Формирование промпта для разных языков.
- Отправляет POST с разными значениями языка (en, fr, de)
- Проверяет, что промпт содержит нужный язык (English, French, German)
- **Сценарий:** Feature — правильная работа выбора языка в форме

### 10. **test_invalid_json_response**
**Что проверяет:** Обработка невалидного JSON от API.
- API возвращает некорректный JSON (например, `{invalid_json`)
- Функция возвращает строку об ошибке (не падает)
- **Сценарий:** API response validation — устойчивость к плохим данным

### 11. **test_get_root_returns_form**
**Что проверяет:** GET-запрос к корневому маршруту.
- Отправляет GET запрос на `/`
- Проверяет статус 200
- Проверяет наличие элементов формы: `<textarea>`, `<select>`, кнопка перевода
- **Сценарий:** Web UI — начальная загрузка страницы

## Структура тестового файла

```
tests/unit/test_all.py
├── Импорты и пути
├── Вспомогательные функции:
│   ├── _start_capture_server()   — создаёт локальный HTTP-сервер
│   ├── _stop_server()            — останавливает сервер
│   └── _reload_app()             — перезагружает модуль app
└── 11 функций тестов (test_*)
```

## Что важно для QA

✅ **Преимущества подхода:**
- Тесты работают максимально близко к реальному поведению (без моков)
- Проверяется не только успешный сценарий, но и обработка ошибок
- Покрываются граничные случаи (пустой ввод, длинный текст, невалидный JSON)
- Проверяется корректность структуры HTTP-запроса (заголовки, тело)
- Тестируется веб-логика через Flask test client (без браузера)

❌ **Что НЕ покрывается:**
- UI-тесты в браузере (используется Flask test client)
- Производительность под нагрузкой
- Совместимость с разными браузерами
- Внешний дизайн и юзабилити

## Как добавить свой тест

1. Откройте файл `tests/unit/test_all.py`
2. Добавьте новую функцию в конец файла:
   ```python
   def test_my_new_feature():
       """Описание что тест проверяет."""
       server, url = _start_capture_server(response_body={'response': 'ok'}, status=200)
       try:
           os.environ['MENTORPIECE_API_URL'] = url
           os.environ['MENTORPIECE_API_KEY'] = 'test-key'
           app = _reload_app()
           
           # Ваша логика теста
           result = app.call_llm('model', 'prompt')
           assert result == 'ok'
       finally:
           _stop_server(server)
   ```
3. Запустите `pytest tests/unit/test_all.py::test_my_new_feature -v`

## Быстрые команды

```bash
# Запустить все тесты с кратким выводом
pytest tests/unit -q

# Запустить с подробным выводом (show all assertions)
pytest tests/unit -vv

# Запустить только тесты с именем, содержащим "api"
pytest tests/unit -k api -v

# Запустить конкретный тест
pytest tests/unit/test_all.py::test_positive_call_llm -v

# Запустить с покрытием кода (требует pytest-cov)
pytest tests/unit --cov=src --cov-report=html
```

---

**Последнее обновление:** December 10, 2025
